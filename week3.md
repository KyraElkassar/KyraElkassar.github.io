---
layout: post
title: Week 3
author: Kyra Elkassar 
---

This week, I was able to gather the key transcriptomic datasets that we will be using to develop our machine learning model. To do this, I used the datasets that the resources we found in the previous week used for their models. I used these datasets because once we build and test our model, we will be able to compare its accuracy to the previous research. We also used the Gene Expression Omnibus to find new datasets. Once we found these datasets, we began the process of data cleaning. Our graduate mentor showed us an example of what a clean transcriptomic dataset looks like. Our main goal this week was to match the transcriptomic datasets that we found to the clean dataset that our mentor gave us. This process was the most difficult part of my week by far. This was the first week that we had to actually apply the Python skills that we learned throughout this program towards our project. A difficulty that I ran into was learning how to transpose, drop columns, and use many more data cleaning techniques that were required to complete this challenge. We did this data cleaning using the Python coding language on Google Colab. When we were done, we saved the data frames as CSVs in Excel. While cleaning the data, we realized that the datasets we found only provided the probe ID of the genes rather than the gene names. To convert the probes into the gene names, we used the BioMart feature in Ensembl ID. With this application, we were able to convert the probe IDs. The highlight of my week was going to Johns Hopkins APL. This trip gave me insight into what I could possibly do in my future. It gave me a new perspective on the field of AI and the depths that go into machine learning and deep learning.
